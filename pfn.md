# 確率的ニューラルネットの学習と Chainer による実装

Speaker : Seiya Tokui

## 概要

* 確率的ニューラルネットワークはbackpropがめんどくさそうに見えるけど実際はそんなには難しくない

## 確率的ニューラルネットワーク

ユニットから送られるデータをパラメータとした，サンプルリングを行う．
例えば，あるユニットからは平均値μ，別のユニットからは分散σ^2をもらって，それらをパラメータとするガウス関数をもとに出力する(連続的)

ベルヌーイ関数を使う場合もある．この場合離散的

### なぜやるか?

* 多クラス分類の分け方を確率的にする．
* 情報なしから画像の生成
  * VAE

### 難しいか?

* 確率で変化するロスを最適化する
* 計算量が膨大

#### approxが必要

### likehood-ratio method

* 普通にforward propする
* ロスが大きかったら，確率を上げる．小さかったら確率を下げる

#### 欠点

* 勾配が大きくずれる
  * Baselineという手法を使う．f(z)の代わりに(f(z) - b)を使う．
    * bは移動平均とか

### Rreparameterization trick

* ガウス関数等連続的な関数にしか使えない
* サンプリング部にεというホワイトノイズの形式にする(μ+σ*ε)
  * 決定論的ニューラルネットワークに帰着できる
